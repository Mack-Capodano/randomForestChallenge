---
title: "Random Forest Challenge"
subtitle: "The Power of Weak Learners"
format:
  html: default
execute:
  echo: false
  eval: true
---

# ðŸŒ² Random Forest Challenge - The Power of Weak Learners

```{r}
#| label: load-and-model-r
#| echo: false
#| message: false
#| warning: false

# Load libraries
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(rpart))

# Load data
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath, 
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  # Convert zipCode to factor (categorical variable) - important for proper modeling
  mutate(zipCode = as.factor(zipCode)) %>%
  na.omit()

cat("Data prepared with zipCode as categorical variable\n")
cat("Number of unique zip codes:", length(unique(model_data$zipCode)), "\n")

# Split data
set.seed(123)
train_indices <- sample(1:nrow(model_data), 0.8 * nrow(model_data))
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# Build random forests with different numbers of trees (with corrected categorical zipCode)
rf_1 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1, mtry = 3, seed = 123)
rf_5 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5, mtry = 3, seed = 123)
rf_25 <- randomForest(SalePrice ~ ., data = train_data, ntree = 25, mtry = 3, seed = 123)
rf_100 <- randomForest(SalePrice ~ ., data = train_data, ntree = 100, mtry = 3, seed = 123)
rf_500 <- randomForest(SalePrice ~ ., data = train_data, ntree = 500, mtry = 3, seed = 123)
rf_1000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1000, mtry = 3, seed = 123)
rf_2000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 2000, mtry = 3, seed = 123)
rf_5000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5000, mtry = 3, seed = 123)
```



```{r}
#| label: performance-comparison-r
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

# Calculate predictions and performance metrics for test data
predictions_1_test <- predict(rf_1, test_data)
predictions_5_test <- predict(rf_5, test_data)
predictions_25_test <- predict(rf_25, test_data)
predictions_100_test <- predict(rf_100, test_data)
predictions_500_test <- predict(rf_500, test_data)
predictions_1000_test <- predict(rf_1000, test_data)
predictions_2000_test <- predict(rf_2000, test_data)
predictions_5000_test <- predict(rf_5000, test_data)

# Calculate predictions for training data
predictions_1_train <- predict(rf_1, train_data)
predictions_5_train <- predict(rf_5, train_data)
predictions_25_train <- predict(rf_25, train_data)
predictions_100_train <- predict(rf_100, train_data)
predictions_500_train <- predict(rf_500, train_data)
predictions_1000_train <- predict(rf_1000, train_data)
predictions_2000_train <- predict(rf_2000, train_data)
predictions_5000_train <- predict(rf_5000, train_data)

# Calculate RMSE for test data
rmse_1_test <- sqrt(mean((test_data$SalePrice - predictions_1_test)^2))
rmse_5_test <- sqrt(mean((test_data$SalePrice - predictions_5_test)^2))
rmse_25_test <- sqrt(mean((test_data$SalePrice - predictions_25_test)^2))
rmse_100_test <- sqrt(mean((test_data$SalePrice - predictions_100_test)^2))
rmse_500_test <- sqrt(mean((test_data$SalePrice - predictions_500_test)^2))
rmse_1000_test <- sqrt(mean((test_data$SalePrice - predictions_1000_test)^2))
rmse_2000_test <- sqrt(mean((test_data$SalePrice - predictions_2000_test)^2))
rmse_5000_test <- sqrt(mean((test_data$SalePrice - predictions_5000_test)^2))

# Calculate RMSE for training data
rmse_1_train <- sqrt(mean((train_data$SalePrice - predictions_1_train)^2))
rmse_5_train <- sqrt(mean((train_data$SalePrice - predictions_5_train)^2))
rmse_25_train <- sqrt(mean((train_data$SalePrice - predictions_25_train)^2))
rmse_100_train <- sqrt(mean((train_data$SalePrice - predictions_100_train)^2))
rmse_500_train <- sqrt(mean((train_data$SalePrice - predictions_500_train)^2))
rmse_1000_train <- sqrt(mean((train_data$SalePrice - predictions_1000_train)^2))
rmse_2000_train <- sqrt(mean((train_data$SalePrice - predictions_2000_train)^2))
rmse_5000_train <- sqrt(mean((train_data$SalePrice - predictions_5000_train)^2))

# Calculate R-squared
r2_1 <- 1 - sum((test_data$SalePrice - predictions_1_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5 <- 1 - sum((test_data$SalePrice - predictions_5_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_25 <- 1 - sum((test_data$SalePrice - predictions_25_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_100 <- 1 - sum((test_data$SalePrice - predictions_100_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_500 <- 1 - sum((test_data$SalePrice - predictions_500_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_1000 <- 1 - sum((test_data$SalePrice - predictions_1000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_2000 <- 1 - sum((test_data$SalePrice - predictions_2000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5000 <- 1 - sum((test_data$SalePrice - predictions_5000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)

# Create performance comparison
performance_df <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  RMSE_Test = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test),
  RMSE_Train = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  R_squared = c(r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000)
)

print(performance_df)
```


### 1. The Power of More Trees Visualization

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 6

# RMSE Plot: Show how RMSE decreases with more trees (both training and test)
plot(performance_df$Trees, performance_df$RMSE_Test,
     type = "b", col = "blue", pch = 19, log = "x",
     xlab = "Number of Trees", ylab = "RMSE",
     main = "RMSE Falls as Number of Trees Increases",
     ylim = range(c(performance_df$RMSE_Test, performance_df$RMSE_Train)))
lines(performance_df$Trees, performance_df$RMSE_Train,
      type = "b", col = "red", pch = 17)
legend("topright", c("Test RMSE", "Train RMSE"),
       col = c("blue", "red"), pch = c(19, 17), lty = 1)
```

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 6

# R-squared Plot: Show how R-squared increases with more trees
plot(performance_df$Trees, performance_df$R_squared,
     type = "b", col = "darkgreen", pch = 19, log = "x",
     xlab = "Number of Trees", ylab = "R-squared",
     main = "R-squared Goes Pp as  Number of Tree Increases ",
     ylim = c(0.6, 0.9))
```


The most dramatic improvement in performance happens as the model jumps from 1 to 5 trees. The improvement in this jump from 1-5 is similar if not larger than the improvement from 5 all the way to 5000. This is true for both the train and test RMSEs and the R-Squared Value. This shows the concept of diminishing returns from adding more trees where each additional tree leads to a smaller and smaller improvement until each indiviudal tree causes no discernible change in the RMSE or R-Squared Value. And the real "valuable" trees in terms of the largest effect on error and r-sqaured are the earliest ones.
:::

### 2. Overfitting Visualization and Analysis

:::

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 6

# Create decision trees with different max depths
max_depths <- c(1, 2, 3, 5, 8, 10, 15, 20, 25, 30)
dt_rmse_train <- numeric(length(max_depths))
dt_rmse_test <- numeric(length(max_depths))

# Train decision trees with different max depths
for (i in 1:length(max_depths)) {
  dt_model <- rpart(SalePrice ~ ., data = train_data,
                    control = rpart.control(maxdepth = max_depths[i], cp = 0))
  dt_pred_train <- predict(dt_model, train_data)
  dt_pred_test <- predict(dt_model, test_data)

  dt_rmse_train[i] <- sqrt(mean((train_data$SalePrice - dt_pred_train)^2))
  dt_rmse_test[i] <- sqrt(mean((test_data$SalePrice - dt_pred_test)^2))
}

# Create side-by-side comparison plots
par(mfrow = c(1, 2))

# Decision Trees: Training vs Test RMSE as max depth increases (showing overfitting)
plot(max_depths, dt_rmse_train, type = "b", col = "red", pch = 17,
     xlab = "Max Depth", ylab = "RMSE",
     main = "Decision Trees: Overfitting with Complexity",
     ylim = range(c(dt_rmse_train, dt_rmse_test, performance_df$RMSE_Train, performance_df$RMSE_Test)))
lines(max_depths, dt_rmse_test, type = "b", col = "blue", pch = 19)
legend("topright", c("Training RMSE", "Test RMSE"),
       col = c("red", "blue"), pch = c(17, 19), lty = 1)

# Random Forests: Training vs Test RMSE as number of trees increases (no overfitting)
plot(performance_df$Trees, performance_df$RMSE_Train, type = "b", col = "red", pch = 17, log = "x",
     xlab = "Number of Trees", ylab = "RMSE",
     main = "Random Forests: Stable Performance",
     ylim = range(c(dt_rmse_train, dt_rmse_test, performance_df$RMSE_Train, performance_df$RMSE_Test)))
lines(performance_df$Trees, performance_df$RMSE_Test, type = "b", col = "blue", pch = 19)
legend("topright", c("Training RMSE", "Test RMSE"),
       col = c("red", "blue"), pch = c(17, 19), lty = 1)
```

For the decision trees the training RMSE starts lower than the test RMSE and quickly becomes much lower as the test begins to actually get worse. This means as the complexity increases, overfitting can start to occur. Overfitting here gets caused as the model starts to memorize the training data and overfits to it, leading to the test RMSE swinging hard and even increasing at times as max depth increases. The random forest shows how both training and test improve as approaching max depth. The train data does wind up far lower then the test showing that you can't justu se your training data as that would be overly optimistic.Random forest don't suffer from overfitting even when increasing trees, because of bootstrap sampling, random feature selection, and averaging the trees. Bootstrap sampling helps prevent overfitting through resampling the data and allowing the model to be re-built on different data. Random feature selection helps prevent overfitting by randomly selecting subsets of features to train on preventing the model from overfitting to one feature. Averaging the trees helps to reduce the variance of the model, preventing overfitting. This is why random forests can avoid overfitting whereas decision trees fail to do so. 



### 3. Linear Regression vs Random Forest Comparison

:::

```{r}
#| echo: false

# Fit linear regression model
lm_model <- lm(SalePrice ~ ., data = train_data)
lm_predictions <- predict(lm_model, test_data)
lm_rmse <- sqrt(mean((test_data$SalePrice - lm_predictions)^2))

# Get RMSE values for random forests with 1, 100, and 1000 trees
rf_1_rmse <- performance_df$RMSE_Test[performance_df$Trees == 1]
rf_100_rmse <- performance_df$RMSE_Test[performance_df$Trees == 100]
rf_1000_rmse <- performance_df$RMSE_Test[performance_df$Trees == 1000]

# Calculate percentage improvements over linear regression
rf_1_improvement <- ((lm_rmse - rf_1_rmse) / lm_rmse) * 100
rf_100_improvement <- ((lm_rmse - rf_100_rmse) / lm_rmse) * 100
rf_1000_improvement <- ((lm_rmse - rf_1000_rmse) / lm_rmse) * 100

# Create comparison table
comparison_df <- data.frame(
  Model = c("Linear Regression", "Random Forest (1 tree)", "Random Forest (100 trees)", "Random Forest (1000 trees)"),
  RMSE = c(lm_rmse, rf_1_rmse, rf_100_rmse, rf_1000_rmse),
  Improvement_vs_LR = c(NA, rf_1_improvement, rf_100_improvement, rf_1000_improvement)
)

# Format the table nicely
comparison_df$RMSE <- round(comparison_df$RMSE, 2)
comparison_df$Improvement_vs_LR <- ifelse(is.na(comparison_df$Improvement_vs_LR),
                                          "",
                                          paste0(round(comparison_df$Improvement_vs_LR, 1), "%"))

# Display the table
knitr::kable(comparison_df,
             col.names = c("Model", "Test RMSE", "% Improvement vs Linear Regression"),
             caption = "Linear Regression vs Random Forest Comparison")
```

The results show that linear regression slightly outperforms all random forest models on this dataset. Random forests are generally expected to perform better when there are nonlinear relationships, but the data here appears fairly homogeneous and linear, which helps the linear model perform best. The 1 tree random forest performs poorly by overfitting, but as the number of trees increases to 100 and 1000, performance becomes comparable to linear regression. The 100 and 1000 tree models reduce error drastically relative to the single tree, though they still fall short of the regression. Overall the added complexity of random forests did not yield a significant gain here. In other datasets random forests likely be expected to outperform, but for this housing data linear regression remains the most efficient and interpretable choice.